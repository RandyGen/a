人工知能（じんこうちのう、、AI〈エーアイ〉）とは、「『計算（）』という概念と『コンピュータ（）』という道具を用いて『知能』を研究する計算機科学（）の一分野」を指す語。「言語の理解や推論、問題解決などの知的行動を人間に代わってコンピューターに行わせる技術」、または、「計算機（コンピュータ）による知的な情報処理システムの設計や実現に関する研究分野」ともされる。

『日本大百科全書(ニッポニカ)』の解説で、情報工学者・通信工学者の佐藤理史は次のように述べている。
人間の知的能力をコンピュータ上で実現する、様々な技術・ソフトウェア・コンピュータシステム。応用例は自然言語処理（機械翻訳・かな漢字変換・構文解析等）、専門家の推論・判断を模倣するエキスパートシステム、画像データを解析して特定のパターンを検出・抽出したりする画像認識等がある。1956年にダートマス会議でジョン・マッカーシーにより命名された。現在では、記号処理を用いた知能の記述を主体とする情報処理や研究でのアプローチという意味あいでも使われている。家庭用電気機械器具の制御システムやゲームソフトの思考ルーチンもこう呼ばれることもある。

プログラミング言語 による「」というカウンセラーを模倣したプログラム（人工無脳）がしばしば引き合いに出されるが、計算機に人間の専門家の役割をさせようという「エキスパートシステム」と呼ばれる研究・情報処理システムの実現は、人間が暗黙に持つ常識の記述が問題となり、実用への利用が困難視されている。人工的な知能の実現へのアプローチとしては、「ファジィ理論」や「ニューラルネットワーク」などのようなアプローチも知られているが、従来の人工知能である (Good Old Fashioned AI) との差は記述の記号的明示性にある。その後「サポートベクターマシン」が注目を集めた。また、自らの経験を元に学習を行う強化学習という手法もある。「この宇宙において、知性とは最も強力な形質である（レイ・カーツワイル）」という言葉通り、知性を機械的に表現し実装するということは極めて重要な作業である。

2006年のディープラーニング（深層学習）の登場と2010年代以降のビッグデータの登場により、一過性の流行を超えて社会に浸透して行った。2016年から2017年にかけて、ディープラーニングを導入したAIが完全情報ゲームである囲碁などのトップ棋士、さらに不完全情報ゲームであるポーカーの世界トップクラスのプレイヤーも破り、麻雀では「Microsoft Suphx (Super Phoenix)」がAIとして初めて十段に到達するなど、時代の最先端技術となった。

第２次人工知能ブームでの人工知能は機械学習と呼ばれ、以下のようなものがある。

一方、計算知能（CI）は開発や学習を繰り返すことを基本としている（例えば、パラメータ調整、コネクショニズムのシステム）。学習は経験に基づく手法であり、非記号的AI、美しくないAI、ソフトコンピューティングと関係している。その手法としては、以下のものがある。
これらを統合した知的システムを作る試みもなされている。ACT-Rでは、エキスパートの推論ルールを、統計的学習を元にニューラルネットワークや生成規則を通して生成する。

第3次人工知能ブームでは、ディープラーニングが画像認識、テキスト解析、音声認識など様々な領域で第2次人工知能ブームの人工知能を上回る精度を出しており、ディープラーニングの研究が盛んに行われている。最近では、DQN、CNN、RNN、GANと様々なディープラーニングの派生がでて各分野で活躍している。特に、GAN（敵対的生成ネットワーク）は、ディープラーニングが認識や予測などの分野で成果をだしていることに加えて、画像の生成技術において大きな進化を見せている。森正弥はこれらの成果を背景に、従来の人工知能の応用分野が広がっており、Creative AIというコンテンツ生成を行っていく応用も始まっていると指摘している。

AIの構築が長い間試みられてきているが、シンボルグラウンディング問題とフレーム問題の解決が大きな壁となってきた。

17世紀初め、ルネ・デカルトは、動物の身体がただの複雑な機械であると提唱した（機械論）。ブレーズ・パスカルは1642年、最初の機械式計算機を製作した。チャールズ・バベッジとエイダ・ラブレスはプログラム可能な機械式計算機の開発を行った。

バートランド・ラッセルとアルフレッド・ノース・ホワイトヘッドは『数学原理』を出版し、形式論理に革命をもたらした。ウォーレン・マカロックとウォルター・ピッツは「神経活動に内在するアイデアの論理計算」と題する論文を1943年に発表し、ニューラルネットワークの基礎を築いた。

1950年代になるとAIに関して活発な成果が出始めた。ジョン・マッカーシーはAIに関する最初の会議で「人工知能」という用語を作り出した。彼はまたプログラミング言語を開発した。知的ふるまいに関するテストを可能にする方法として、アラン・チューリングは「チューリングテスト」を導入した。ジョセフ・ワイゼンバウムはを構築した。これは来談者中心療法を行うおしゃべりロボットである。

1956年に行われた、ダートマス会議開催の提案書において、人類史上、用語として初めて使用され、新たな分野として創立された。

1960年代と1970年代の間に、ジョエル・モーゼスは プログラム中で積分問題での記号的推論のパワーを示した。マービン・ミンスキーとシーモア・パパートは『パーセプトロン』を出版して単純なニューラルネットの限界を示し、アラン・カルメラウアーはプログラミング言語 を開発した。テッド・ショートリッフェは医学的診断と療法におけるルールベースシステムを構築し、知識表現と推論のパワーを示した。これは、最初のエキスパートシステムと呼ばれることもある。ハンス・モラベックは、障害物の
あるコースを自律的に走行する最初のコンピューター制御の乗り物を開発した。

1980年代に、ニューラルネットワークはバックプロパゲーションアルゴリズムによって広く使われるようになった。

また、この時代にロドニー・ブルックスが、知能には身体が必須との学説（身体性）を提唱した。

1990年代はAIの多くの分野で様々なアプリケーションが成果を上げた。特に、ボードゲームでは目覚ましく、1992年にIBMは世界チャンピオンに匹敵するバックギャモン専用コンピュータ・TDギャモンを開発し、IBMのチェス専用コンピュータ・ディープ・ブルーは、1997年5月にガルリ・カスパロフを打ち負かし、同年8月にはオセロで日本電気のオセロ専用コンピュータ・ロジステロに世界チャンピオンの村上健が敗れた。国防高等研究計画局は、最初の湾岸戦争においてユニットをスケジューリングするのにAIを使い、これによって省かれたコストが1950年代以来のAI研究への政府の投資全額を上回ったことを明らかにした。日本では甘利俊一（日本学士院会員）らが精力的に啓蒙し、優秀な成果も発生したが、論理のブラックボックス性が指摘された。

1998年には非構造化データ形式の国際規格であるXMLが提唱されたが、ここからWeb上の非構造化データに対して、アプリケーション別に適した意味付けを適用し、処理を行わせる試みが開始された。同年に、W3Cのティム・バーナーズ＝リーにより、Webに知的処理を行わせるセマンティック・ウェブが提唱された。この技術はWeb上のデータに意味を付加して、コンピュータに知的処理を行わせる方法を国際的に規格化するものである。この規格には知識工学におけるオントロジーを表現するデータ形式のOWLも含まれていることから、かつて流行したエキスパートシステムの亜種であることが分かる。2000年代前半に規格化が完了しているが、Web開発者にとっては開発工数に見合うだけのメリットが見出せなかったことから、現在も普及はしていない。

日本においてはエキスパートシステムの流行の後にニューロファジィが流行した。しかし、研究が進むにつれて計算リソースやデータ量の不足，シンボルグラウンディング問題，フレーム問題に直面し、産業の在り方を激変させるようなAIに至ることは無く、ブームは終焉した。

1980年代に入って、大企業の研究所を中心に、知識工学に基づくエキスパートシステムが多数提案されるようになり、エキスパートシステムを専門とするAIベンチャーも次々と立ち上がった。その流行から生まれた究極のプロジェクトとして第五世代コンピュータが挙げられる。

1982年から1992年まで日本は国家プロジェクトとして570億円を費やして第五世代コンピュータの研究を進めるも、採用した知識工学的手法では膨大なルールの手入力が必要で、専門家間で専門知識の解釈が異なる場合には統一したルール化が行えない等の問題もあり、実用的なエキスパートシステムの実現には至らなかった。実現した成果物はPrologの命令を直接CPUのハードウェアの機構で解釈して高速に実行する、並列型のProlog専用機であるが、商業的な意味で応用先が全く見つからなかった。

1980年代後半から1990年代中頃にかけて、従来から電子制御の手法として用いられてきたON/OFF制御，PID制御，現代制御の問題を克服するため、知的制御が盛んに研究され、知識工学的なルールを用いるファジィ制御，データの特徴を学習して分類するニューラルネットワーク，その2つを融合したニューロファジィという手法が日本を中心にブームを迎えた。バブル期の高級路線に合わせて、白物家電製品でもセンサの個数と種類を大幅に増やし、多様なデータを元に運転を最適化するモデルが多数発売され始めた。

ファジィについては、2018年までに日本が世界の1/5の特許を取得している事から、日本で特に大きなブームとなっていたことが分かっている。現在の白物家電ではこの当時より更に発展した制御技術が用いられているが、既に当たり前のものになり、利用者には意識されなくなっている。ニューロファジィがブームになった1990年代には未だビッグデータという概念は無く（ブロードバンド接続普及後の2010年に初めて提唱された）、データマイニングとしての産業応用は行われなかった。しかし、ニューラルネットワークが一般人も巻き込んで流行した事例としては初めての事例であり、2010年代のディープラーニングブームの前史とも言える社会現象と言える。

松下電器が1985年頃から人間が持つような曖昧さを制御に活かすファジィ制御についての研究を開始し、1990年2月1日にファジィ洗濯機第1号である「愛妻号Dayファジィ」の発売に漕ぎ着けた。「愛妻号Dayファジィ」は従来よりも多数のセンサーで収集したデータに基づいて、柔軟に運転を最適化する洗濯機で、同種の洗濯機としては世界初であった。ファジィ制御という当時最先端の技術の導入がバブル期の高級路線にもマッチしたことから、ファジィは裏方の制御技術であるにも関わらず世間の大きな注目を集めた。その流行の度合いは、1990年の新語・流行語大賞における新語部門の金賞で「ファジィ」が選ばれる程であった。その後に、松下電器はファジィルールの煩雑なチューニングを自動化したニューロファジィ制御を開発し、従来のファジィ理論の限界を突破して学会で評価されるだけでなく、白物家電への応用にも成功して更なるブームを巻き起こした。松下電器の試みの成功を受けて、他社も同様の知的制御を用いる製品を多数発売した。1990年代中頃までは、メーカー各社による一般向けの白物家電の売り文句として知的制御技術の名称が大々的に用いられており、洗濯機の製品名では「愛妻号DAYファジィ」，掃除機の分類としては「ニューロ・ファジィ掃除機」，エアコンの運転モードでは「ニューロ自動」などの名称が付与されていた。

ニューロ，ファジィ，ニューロファジィという手法は、従来の単純なオン・オフ制御や、対象を数式で客観的にモデル化する（この作業は対象が複雑な機構を持つ場合は極めて難しくなる）必要があるPID制御や現代制御等と比較して、人間の主観的な経験則や計測したデータの特徴が利用可能となるファジィ、ニューロ、ニューロファジィは開発工数を抑えながら、環境適応時の柔軟性を高くできるという利点があった。しかし、開発者らの努力にも関わらず、計算能力や収集可能なデータ量の少なさから、既存の工作機械や家電製品の制御を多少改善する程度で限界を迎えた。理論的にもファジィ集合と深層学習が不可能なニューラルネットワークの組み合わせであり、計算リソースやデータが潤沢に与えられたとしても、認識精度の向上には限界があった。

以降、計算機の能力限界から理論の改善は遅々として進まず、目立った進展は無くなり、1990年代末には知的制御を搭載する白物家電が大多数になったことで、売り文句としてのブームは去った。ブーム後は一般には意識されなくなったが、現在では裏方の技術として、家電製品のみならず、雨水の排水，駐車場，ビルの管理システムなどの社会インフラにも使われ、十分に性能と安定性が実証されている。2003年頃には、人間が設計したオントロジー（ファジィルールとして表現する）を利活用するネットワーク・インテリジェンスという分野に発展した。

2005年、レイ・カーツワイルは著作で、「圧倒的な人工知能が知識・知能の点で人間を超越し、科学技術の進歩を担い世界を変革する技術的特異点（シンギュラリティ）が2045年にも訪れる」とする説を発表した。

2006年に、ジェフリー・ヒントンらの研究チームによりオートエンコーダによるニューラルネットワークの深層化手法が提案された（現在のディープラーニングの直接的な起源）。

2010年代に入り、膨大なデータを扱う研究開発のための環境が整備されたことで、AI関連の研究が再び大きく前進し始めた。

2010年に英国エコノミスト誌で「ビッグデータ」という用語が提唱された。同年に質問応答システムのワトソンが、クイズ番組「ジェパディ!」の練習戦で人間に勝利し、大きなニュースとなった。2012年に画像処理コンテストでジェフリー・ヒントン氏のチームが従来手法からの大幅な精度改善を果たした上で優勝したことで、第三次AIブームが始まった。

2013年には国立情報学研究所や富士通研究所の研究チームが開発した「東ロボくん」で東京大学入試の模擬試験に挑んだと発表した。数式の計算や単語の解析にあたる専用プログラムを使い、実際に受験生が臨んだ大学入試センター試験と東大の2次試験の問題を解読した。代々木ゼミナールの判定では「東大の合格は難しいが、私立大学には合格できる水準」だった。

2014年には、日本の人工知能学者である齊藤元章により、特異点に先立ち、オートメーション化とコンピューター技術の進歩により衣食住の生産コストがゼロに限りなく近づくというプレ・シンギュラリティという概念も提唱された。

ジェフ・ホーキンスが、実現に向けて研究を続けているが、著書『考える脳 考えるコンピューター』の中で自己連想記憶理論という独自の理論を展開している。

世界各国において、軍事・民間共に実用化に向け研究開発が進んでいるが、とくに無人戦闘機や無人自動車ロボットカーの開発が進行しているものの、2010年代にはまだ完全な自動化は試験的なものに留まった（UCAVは利用されているが、一部操作は地上から行っているものが多い）。

ロボット向けとしては、CSAILのロドニー・ブルックスが提唱した包摂アーキテクチャという理論が登場している。これは従来型の「我思う、故に我あり」の知が先行するものではなく、体の神経ネットワークのみを用いて環境から学習する行動型システムを用いている。これに基づいたゲンギスと呼ばれる六本足のロボットは、いわゆる「脳」を持たないにも関わらず、まるで生きているかのように行動する。

2015年10月に米DeepMind社が作成した「AlphaGo」が人間のプロ囲碁棋士に勝利して以降はディープラーニングと呼ばれる手法が注目され、人工知能自体の研究の他にも、人工知能が雇用などに与える影響についても研究が進められている。

2016年10月、DeepMindが、入力された情報の関連性を導き出し仮説に近いものを導き出す人工知能技術「ディファレンシャブル・ニューラル・コンピューター」を発表し、同年11月、大量のデータが不要の「ワンショット学習」を可能にする深層学習システムを、翌2017年6月、関係推論のような人間並みの認識能力を持つシステムを開発。2017年8月には、記号接地問題(シンボルグラウンディング問題)を解決した。

2006年のディープラーニングの発明と、2010年以降のビッグデータ収集環境の整備、計算資源となるGPUの高性能化により、2012年にディープラーニングが画像処理コンテストで他の手法に圧倒的大差を付けて優勝したことで、技術的特異点という概念は急速に世界中の識者の注目を集め、現実味を持って受け止められるようになった。ディープラーニングの発明と急速な普及を受けて、研究開発の現場においては、デミス・ハサビス率いるDeepMindを筆頭に、Vicarious、IBM Cortical Learning Center、全脳アーキテクチャ、PEZY Computing、OpenCog、GoodAI、nnaisense、IBM SyNAPSE等、汎用人工知能（AGI）を開発するプロジェクトが数多く立ち上げられている。これらの研究開発の現場では、脳をリバースエンジニアリングして構築された神経科学と機械学習を組み合わせるアプローチが有望とされている。結果として、Hierarchical Temporal Memory (HTM) 理論、Complementary Learning Systems (CLS) 理論の更新版等、単一のタスクのみを扱うディープラーニングから更に一歩進んだ、複数のタスクを同時に扱う理論が提唱され始めている。

3Dゲームのような仮想空間でモデルを動かし現実世界のことを高速に学ばせるといったことも大きな成果を上げている（シミュレーションによる学習）。

また、数は少ないがAGIだけでは知能の再現は不可能と考えて、身体知を再現するために、全人体シミュレーションが必要だとする研究者やより生物に近い振る舞いを見せるAL（人工生命）の作成に挑む研究者、知能と密接な関係にあると思われる意識のデジタル的再現（人工意識）に挑戦する研究者もいる。

リーズナブルなコストで大量の計算リソースが手に入るようになったことで、ビッグデータが出現し、企業が膨大なデータの活用に極めて強い関心を寄せており、全世界的に民間企業主導で莫大な投資を行って人工知能に関する研究開発競争が展開されている。また、2011年のD-Wave Systemsによる量子アニーリング方式の製品化を嚆矢として、量子コンピュータという超々並列処理が可能な次世代のITインフラが急速に実用化され始めた事で、人工知能の高速化にも深く関わる組み合わせ最適化問題をリアルタイムに解決できる環境が整備され始めている。この動向を受ける形で、2016年頃から、一般向けのニュース番組でも人工知能の研究開発や新しいサービス展開や量子コンピュータに関する報道が目立つようになった。

2017年にはイーロン・マスクが、急速に進化し続ける人工知能に対して人間が遅れを取らないようにするために、人間の脳を機械に接続するブレイン・マシン・インターフェースを研究開発するニューラ・リンク社を立ち上げていたことを公表し、世界中で話題になった。ブレイン・マシン・インターフェースにより、人のインターネットが出現する事が予測されている。

2017年10月にはジェフリー・ヒントンにより要素間の相対的な位置関係まで含めて学習できるカプセルネットワークが提唱された。

2018年3月16日の国際大学GLOCOMの提言によると、課題解決型のAIを活用する事で社会変革に寄与できると分析されている。

2018年8月、Open AIが好奇心を実装しノーゲームスコア、ノーゴール、無報酬で目的なき探索を行うAIを公表。これまでのAIで最も人間らしいという。
2018年9月、MITリンカーン研究所は従来ブラックボックスであったニューラルネットワークの推論をどのような段階を経て識別したのかが明確に分かるアーキテクチャを開発した。

2019年に入るとこれまで深層学習では困難とされてきた言語処理において大きな進展があり、Wikipediaなどを使用した読解テストで人間を上回るに至った。(BERT、ROBERT。)
